"""YOLO Model for 2D Object Detection on Multi-Camera Images."""

import torch
import torch.nn as nn
import torch.nn.functional as F


class YOLO(nn.Module):
    """
    YOLO model for 2D object detection on multi-camera images.
    
    Takes images from a batch generated by loader_2d_obj_det.py and produces
    predicted boxes and labels.
    
    Input:
        images: torch.Tensor of shape (batch_size, N_cam, C, H, W)
                - batch_size: number of samples in batch
                - N_cam: number of cameras (typically 6)
                - C: channels (3 for RGB)
                - H, W: height and width (typically 640x640)
    
    Output:
        predictions: dict with keys:
            'boxes': torch.Tensor of shape (batch_size * N_cam, N_anchors, 4)
                     - YOLO format: [x_center, y_center, width, height] (normalized)
            'labels': torch.Tensor of shape (batch_size * N_cam, N_anchors, N_classes)
                     - Class logits for each anchor
            'objectness': torch.Tensor of shape (batch_size * N_cam, N_anchors, 1)
                         - Objectness scores
    """
    
    def __init__(self, cfg, num_classes=None, img_size=None, backbone=None, anchor_sizes=None, rank=0):
        """
        Initialize YOLO model.
        
        Args:
            cfg: Configuration dictionary containing model settings under 'Yolo' key.
                 Expected keys: 'num_classes', 'img_size', 'backbone', 'anchor_sizes'
            num_classes: Number of object classes (optional, overrides cfg if provided)
            img_size: Input image size (optional, overrides cfg if provided)
            backbone: Backbone architecture (optional, overrides cfg if provided)
            anchor_sizes: List of anchor sizes for detection (optional, overrides cfg if provided)
        """
        super(YOLO, self).__init__()
        
        # Extract configuration from cfg['Yolo'] if available
        yolo_cfg = cfg.get('Yolo', {}) if isinstance(cfg, dict) else {}
        
        # Use provided arguments or fall back to cfg, then to defaults
        self.num_classes = num_classes if num_classes is not None else yolo_cfg.get('num_classes', 10)
        self.img_size = img_size if img_size is not None else yolo_cfg.get('img_size', 640)
        backbone = backbone if backbone is not None else yolo_cfg.get('backbone', 'resnet18')
        anchor_sizes = anchor_sizes if anchor_sizes is not None else yolo_cfg.get('anchor_sizes', None)
        
        # Default anchor sizes (for 640x640 images)
        if anchor_sizes is None:
            # Multi-scale anchors: small, medium, large
            self.anchor_sizes = [
                [(10, 13), (16, 30), (33, 23)],  # Small objects
                [(30, 61), (62, 45), (59, 119)],  # Medium objects
                [(116, 90), (156, 198), (373, 326)]  # Large objects
            ]
        else:
            self.anchor_sizes = anchor_sizes
        
        self.num_anchors_per_scale = len(self.anchor_sizes[0])
        self.num_scales = len(self.anchor_sizes)
        
        # Build backbone
        self.backbone = self._build_backbone(backbone)
        
        # Detection heads for each scale
        self.detection_heads = nn.ModuleList([
            self._build_detection_head(backbone, scale_idx)
            for scale_idx in range(self.num_scales)
        ])

        # Model size analysis
        if rank == 0:
            self.print_model_summary()

    def print_model_summary(self):
        '''Print model modules with their names and parameter counts (in millions).'''
        print("\n" + "=" * 60)
        print(f"{'Module Name':<40} {'Params (M)':>10} {'Train (M)':>10}")
        print("=" * 60)

        total_params, trainable_params = 0, 0
        for name, module in self.named_children():
            params = sum(p.numel() for p in module.parameters())
            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            total_params += params
            trainable_params += trainable
            print(f"{name:<40} {params/1e6:>10.3f} {trainable/1e6:>10.3f}")

        print("-" * 60)
        print(f"{'TOTAL':<40} {total_params/1e6:>10.3f} {trainable_params/1e6:>10.3f}")
        print("=" * 60 + "\n")
        
    def _build_backbone(self, backbone_name):
        """Build backbone network."""
        if backbone_name == 'resnet18':
            from torchvision.models import resnet18
            backbone = resnet18(pretrained=True)
            # Remove final fully connected layer
            backbone = nn.Sequential(*list(backbone.children())[:-2])
        elif backbone_name == 'resnet34':
            from torchvision.models import resnet34
            backbone = resnet34(pretrained=True)
            backbone = nn.Sequential(*list(backbone.children())[:-2])
        elif backbone_name == 'resnet50':
            from torchvision.models import resnet50
            backbone = resnet50(pretrained=True)
            backbone = nn.Sequential(*list(backbone.children())[:-2])
        else:
            raise ValueError(f"Unsupported backbone: {backbone_name}")
        
        return backbone
    
    def _build_detection_head(self, backbone_name, scale_idx):
        """Build detection head for a specific scale."""
        if backbone_name in ['resnet18', 'resnet34']:
            in_channels = 512
        elif backbone_name == 'resnet50':
            in_channels = 2048
        else:
            in_channels = 512
        
        # Feature pyramid network (FPN) style
        # Reduce channels and create multi-scale features
        if scale_idx == 0:
            # Small scale (high resolution)
            out_channels = 256
        elif scale_idx == 1:
            # Medium scale
            out_channels = 256
        else:
            # Large scale (low resolution)
            out_channels = 256
        
        # Channel reduction
        conv_reduce = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)
        
        # Detection head: predicts boxes, objectness, and class logits
        # Output: (batch, anchors * (4 + 1 + num_classes), H, W)
        num_outputs = self.num_anchors_per_scale * (4 + 1 + self.num_classes)
        conv_det = nn.Conv2d(out_channels, num_outputs, kernel_size=3, stride=1, padding=1)
        
        return nn.Sequential(conv_reduce, conv_det)
    
    def forward(self, images):
        """
        Forward pass through YOLO model.
        
        Args:
            images: torch.Tensor of shape (batch_size, N_cam, C, H, W)
        
        Returns:
            predictions: dict with 'boxes', 'labels', 'objectness'
        """
        batch_size, n_cam, C, H, W = images.shape
        
        # Reshape to process all cameras: (batch_size * N_cam, C, H, W)
        images_flat = images.view(batch_size * n_cam, C, H, W)
        
        # Extract features using backbone
        features = self.backbone(images_flat)
        
        # Process through detection heads at different scales
        all_boxes = []
        all_objectness = []
        all_labels = []
        
        for scale_idx, detection_head in enumerate(self.detection_heads):
            # Get detection outputs
            det_output = detection_head(features)
            
            # Reshape output: (batch * n_cam, anchors, 4+1+num_classes, H, W)
            num_anchors = self.num_anchors_per_scale
            det_output = det_output.view(
                batch_size * n_cam, 
                num_anchors, 
                4 + 1 + self.num_classes, 
                det_output.shape[2], 
                det_output.shape[3]
            )
            
            # Permute to: (batch * n_cam, anchors, H, W, 4+1+num_classes)
            det_output = det_output.permute(0, 1, 3, 4, 2)
            
            # Get grid dimensions for this scale
            grid_h, grid_w = det_output.shape[2], det_output.shape[3]
            
            # Split into box offsets, objectness, and class logits
            box_offsets = det_output[..., :4]  # (batch * n_cam, anchors, H, W, 4) - [tx, ty, tw, th]
            objectness = det_output[..., 4:5]  # (batch * n_cam, anchors, H, W, 1)
            labels = det_output[..., 5:]  # (batch * n_cam, anchors, H, W, num_classes)
            
            # Decode box offsets using anchors
            # box_offsets contains: [tx, ty, tw, th] where:
            # - tx, ty: center offsets (apply sigmoid to get [0, 1] offset within grid cell)
            # - tw, th: size scaling factors (apply exp to scale anchor size)
            decoded_boxes = self._decode_boxes(
                box_offsets, scale_idx, grid_h, grid_w, H, W
            )  # (batch * n_cam, anchors, H, W, 4) - decoded boxes in normalized [0,1] format
            
            # Apply sigmoid to objectness
            objectness = torch.sigmoid(objectness)
            
            # Reshape to: (batch * n_cam, anchors * H * W, ...)
            grid_size = grid_h * grid_w
            decoded_boxes = decoded_boxes.reshape(batch_size * n_cam, num_anchors * grid_size, 4)
            objectness = objectness.reshape(batch_size * n_cam, num_anchors * grid_size, 1)
            labels = labels.reshape(batch_size * n_cam, num_anchors * grid_size, self.num_classes)
            
            all_boxes.append(decoded_boxes)
            all_objectness.append(objectness)
            all_labels.append(labels)
        
        # Concatenate predictions from all scales
        predictions = {
            'boxes': torch.cat(all_boxes, dim=1),  # (batch * n_cam, total_anchors, 4)
            'objectness': torch.cat(all_objectness, dim=1),  # (batch * n_cam, total_anchors, 1)
            'labels': torch.cat(all_labels, dim=1),  # (batch * n_cam, total_anchors, num_classes)
        }
        
        return predictions
    
    def _decode_boxes(self, box_offsets, scale_idx, grid_h, grid_w, img_h, img_w):
        """
        Decode anchor-relative offsets to absolute normalized box coordinates.
        
        Args:
            box_offsets: torch.Tensor (batch_size, num_anchors, H, W, 4) - [tx, ty, tw, th]
            scale_idx: int - scale index (0=small, 1=medium, 2=large)
            grid_h: int - feature map height (output from backbone)
            grid_w: int - feature map width (output from backbone)
            img_h: int - input image height (kept for consistency, not directly used in current formula)
            img_w: int - input image width (kept for consistency, not directly used in current formula)
        
        Returns:
            decoded_boxes: torch.Tensor (batch_size, num_anchors, H, W, 4)
                          - Decoded boxes in YOLO format [x_center, y_center, width, height] (normalized to [0,1] relative to input image)
        
        Note:
            The formula (grid_x + x_offset) / grid_w correctly converts feature map coordinates
            to normalized input image coordinates [0, 1] because stride = img_size / grid_size.
            This ensures consistency between grid coordinates (feature map space) and anchor sizes
            (input image space), both normalized to [0, 1] relative to input image.
        """
        device = box_offsets.device
        _ = img_h, img_w  # Kept for API consistency and potential future validation
        
        # Note: grid_w and grid_h are feature map dimensions (e.g., 20x20 if input is 640x640 with stride 32)
        # anchor_sizes_norm is normalized by input image size (img_size)
        # The decoding formula (grid_x + x_offset) / grid_w correctly converts feature map
        # coordinates to normalized input image coordinates [0, 1] because:
        # stride = img_size / grid_size, so (grid_x + x_offset) / grid_w = (grid_x + x_offset) * stride / img_size
        
        # Get anchor sizes for this scale
        anchors = self.anchor_sizes[scale_idx]  # List of (w, h) tuples
        
        # Normalize anchor sizes to [0, 1] range relative to input image
        anchor_sizes_norm = torch.tensor(
            [[w / self.img_size, h / self.img_size] for w, h in anchors],
            device=device, dtype=box_offsets.dtype
        )  # (num_anchors, 2) - [w_norm, h_norm] relative to input image
        
        # Split offsets
        tx = box_offsets[..., 0]  # (batch_size, num_anchors, H, W)
        ty = box_offsets[..., 1]
        tw = box_offsets[..., 2]
        th = box_offsets[..., 3]
        
        # Create grid coordinates (integer positions: 0, 1, 2, ..., grid_w-1)
        # These are in feature map space
        y_coords = torch.arange(grid_h, device=device, dtype=box_offsets.dtype)
        x_coords = torch.arange(grid_w, device=device, dtype=box_offsets.dtype)
        grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
        
        # Expand grid for broadcasting: (1, 1, H, W)
        grid_x = grid_x.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W) - in feature map space
        grid_y = grid_y.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W) - in feature map space
        
        # Expand anchor sizes for broadcasting: (1, num_anchors, 1, 1)
        anchor_w_norm = anchor_sizes_norm[:, 0].unsqueeze(0).unsqueeze(-1).unsqueeze(-1)  # (1, num_anchors, 1, 1)
        anchor_h_norm = anchor_sizes_norm[:, 1].unsqueeze(0).unsqueeze(-1).unsqueeze(-1)  # (1, num_anchors, 1, 1)
        
        # Decode center coordinates
        # sigmoid(tx) gives offset [0, 1] within grid cell
        x_offset = torch.sigmoid(tx)  # (batch_size, num_anchors, H, W) - offset within cell [0, 1]
        y_offset = torch.sigmoid(ty)
        
        # Convert from feature map space to input image space (normalized to [0, 1])
        # Position in pixels: (grid_x + x_offset) * stride_w
        # Normalized: (grid_x + x_offset) * stride_w / img_w
        # Since stride_w = img_w / grid_w, this simplifies to: (grid_x + x_offset) / grid_w
        # This gives us normalized position [0, 1] relative to input image
        x_center = (grid_x + x_offset) / grid_w  # Normalized [0, 1] relative to input image
        y_center = (grid_y + y_offset) / grid_h  # Normalized [0, 1] relative to input image
        
        # Decode width and height: w = anchor_w * exp(tw) / img_size
        # exp(tw) gives scaling factor, multiply by anchor size (already normalized)
        # Clamp tw/th to prevent exp() from exploding (limit to reasonable range: -5 to 2)
        # This limits scaling factor to approximately [0.007, 7.4]
        tw_clamped = torch.clamp(tw, min=-5.0, max=2.0)
        th_clamped = torch.clamp(th, min=-5.0, max=2.0)
        w_scale = torch.exp(tw_clamped)  # (batch_size, num_anchors, H, W) - learnable scaling factor
        h_scale = torch.exp(th_clamped)
        
        # Decoded size: anchor size * scale factor (already normalized to input image)
        width = anchor_w_norm * w_scale  # Normalized [0, 1] relative to input image
        height = anchor_h_norm * h_scale
        
        # Ensure width and height are positive and reasonable (clamp to [0.01, 1.0] to avoid zero/negative)
        width = torch.clamp(width, min=0.01, max=1.0)
        height = torch.clamp(height, min=0.01, max=1.0)
        
        # Stack into box format: [x_center, y_center, width, height]
        decoded_boxes = torch.stack([x_center, y_center, width, height], dim=-1)
        
        # Clamp all coordinates to valid range [0, 1] and check for NaN/Inf
        decoded_boxes = torch.clamp(decoded_boxes, min=0.0, max=1.0)
        
        # Replace any NaN/Inf with zeros (safety check)
        decoded_boxes = torch.where(
            torch.isfinite(decoded_boxes),
            decoded_boxes,
            torch.zeros_like(decoded_boxes)
        )
        
        return decoded_boxes
    
